# 2024年3月17日_生成AIとAWSでなんかやりたいを具体化

## 時間

- 3/16 13:15-14:45(1h30m)
- 3/17 16:30-17:20(50m)

## やること

- 大規模言語モデル  x AWSでなんかやりたいを具体化

## 備忘録

- 大規模言語モデル  x AWSでなんかやりたい
  - できればAPIをただ呼ぶのではなく、ホスティングまでやりたい

- 大規模言語モデルは、基本的にはアーキテクチャとデータセットを組み合わせたものと考えられるらしい
  - 参考: 百花繚乱の大規模言語モデル　その現状まとめ【2023年4月末版】 https://www.itmedia.co.jp/news/articles/2304/25/news156.html
    - モデル名 アーキテクチャ データセット 規模（パラメータ数） 提供元 利用条件 備考
    - Dolly-v2	Transformer	Databricks社独自の会話データセット	7B-13B	Databricks	オープン、商用利用可
    - Llama 2 Transformer 7B - 70B Meta オープン、商用利用可 Llama 2は日本語にも対応しているが、事前学習データの約90％は英語が占めており、日本語の割合は全体の約0.1％。動作環境について。パラメータ数7BのバージョンGPUメモリ（VRAM）が16GBのローカルPCで利用できます
    - StableLM	Transformer	拡張したPile	7B-13B	Stability.ai	オープン、商用利用可
    - RedPajama	Transformer	RedPajama Dataset	13B（学習中）	Together	オープン、商用利用可
    - RWKV	RWKV	Pile	7B-13B	BlinkDL	オープン、商用利用可
- ここでいうアーキテクチャとは、Transformer、RWKV（ルワクフ）
    - ほとんどの大規模言語モデルはTransformerらしい。
    - Transformer, Hugging Faceで使い方を見たことがある。
      - そもそも、Transformerアーキテクチャとは？ https://crystal-method.com/blog/transformer-2/
        - Transformerとは、2017年に発表された”Attention Is All You Need”という自然言語処理に関する論文の中で初めて登場した深層学習モデル
          - **Transformerアーキテクチャに沿った、オープン、商用利用可能な大規模言語モデルを自分でホスティングしたいと置き換えよう**

- オープン、商用利用可能な大規模言語モデルを動かしてる技術記事を探す
 - dolly-japanese-gpt-1bを動かしてる人を見かけた https://qiita.com/sakue_103/items/8b32059629c01d0d27f2
    - 最低メモリ要件は7GB, GPU無しでもOK
    - 推論時間は7秒程度。
    - もしかして...こいつ...AWSでも動くのか！？
    - HuggingFaceのサイトはこっち
      - https://huggingface.co/inu-ai/dolly-japanese-gpt-1b

- ホスティングはAWS上に実施したい。なるべくエンジニアリングコストのかからないものを選んで実装したい
    - Lambda を使って深層学習モデルをホストする例が存在する
        - https://aws.amazon.com/jp/blogs/news/serving-deep-learning-at-curalate-with-apache-mxnet-aws-lambda-and-amazon-elastic-inference/
            - Lambda 関数に割り当てられるメモリの量は、128 MB から 10,240 MB の間
                - 128MB - 10GB
            - 単一の AWS Lambda 関数には GPU がなく、ごくわずかなコンピューティングリソースしか持たない
            - API Gatewayのバックエンドで動作させる場合タイムアウト時間は最大でも30秒。
            - Lambda 関数は、AWS で深層学習モデルを実行する際に選択できる中で、最も低速なオプション
    - Langchainの実装を確認したところ、APIGatewayでLLMをホスティングするケースに対応するAPIがわざわざ提供されている
        - langchain_community.llms.AmazonAPIGateway https://python.langchain.com/docs/integrations/llms/amazon_api_gateway

- まとめると
  - API Gateway/Lambdaを使ってオープンソースの言語モデル「dolly-japanese-gpt-1b」をホスティングしてみる
    - Transformerアーキテクチャに沿った、オープン、商用利用可能な大規模言語モデル

### ソースコード

#### terraform(main.tf)

```terraform
resource "aws_api_gateway_rest_api" "api" {
  provider = aws.app_local

  name = "mlapp-${terraform.workspace}-api"

  endpoint_configuration {
    types = ["REGIONAL"]
  }
  disable_execute_api_endpoint = true
  tags = local.tags
}

resource "aws_api_gateway_rest_api_policy" "api_policy" {
  provider = aws.app_local

  rest_api_id = aws_api_gateway_rest_api.api.id
  policy = templatefile("${path.module}/templates/api-gateway-policy.json", { })
}

resource "aws_api_gateway_resource" "resource" {
  provider = aws.app_local

  parent_id   = aws_api_gateway_rest_api.api.root_resource_id
  rest_api_id = aws_api_gateway_rest_api.api.id
  path_part   = "{proxy+}"
}

resource "aws_api_gateway_method" "method" {
  provider = aws.app_local

  rest_api_id   = aws_api_gateway_rest_api.api.id
  resource_id   = aws_api_gateway_resource.resource.id
  http_method   = "ANY"

  authorization = "NONE"
}

resource "aws_api_gateway_integration" "integration" {
  provider = aws.app_local

  rest_api_id = aws_api_gateway_rest_api.api.id
  resource_id = aws_api_gateway_resource.resource.id
  http_method = aws_api_gateway_method.method.http_method

  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.lambda.invoke_arn
}

resource "aws_api_gateway_domain_name" "this" {
  provider = aws.app_local

  domain_name              = var.rest_api_domain_name
  regional_certificate_arn = var.certificate_arn

  endpoint_configuration {
    types = ["REGIONAL"]
  }
}
resource "aws_api_gateway_base_path_mapping" "this" {
  provider = aws.app_local

  api_id      = aws_api_gateway_rest_api.api.id
  domain_name = aws_api_gateway_domain_name.this.domain_name
  stage_name  = terraform.workspace
  base_path = ""
}

resource "aws_api_gateway_method_settings" "this" {
  provider = aws.app_local

  rest_api_id = aws_api_gateway_rest_api.api.id
  stage_name  = terraform.workspace
  method_path = "*/*"

  settings {
    metrics_enabled = true
    logging_level   = "INFO"
  }
}

resource "aws_api_gateway_deployment" "deployment" {
  provider = aws.app_local

  rest_api_id = aws_api_gateway_rest_api.api.id
  stage_name  = terraform.workspace
  variables = {
    deployed_at = var.app_deployed_at
  }

  depends_on = [
    aws_api_gateway_integration.integration
  ]

  lifecycle {
    create_before_destroy = true
  }
}
resource "aws_api_gateway_stage" "stage" {
  provider = aws.app_local

  stage_name    = terraform.workspace
  rest_api_id   = aws_api_gateway_rest_api.api.id
  deployment_id = aws_api_gateway_deployment.deployment.id
}

resource "aws_lambda_function" "lambda" {
  provider = aws.app_local

  s3_bucket     = var.backend_deploy_s3_bucket_name
  s3_key        = local.backend_deploy_artifact_s3_key
  function_name = "mlapp-${terraform.workspace}-api"
  handler       = "yes"
  role          = aws_iam_role.api_lambda_role.arn
  timeout       = 30
  memory_size   = 7168
  // The runtime of the lambda function
  runtime       = "provided.al2"

  environment {
    variables = {
      ENV = terraform.workspace
    }
  }
  depends_on = [
    aws_s3_object.app_archive_upload
  ]

  tags = local.tags
}
resource "aws_s3_object" "app_archive_upload" {
  provider = aws.app_local

  bucket = var.backend_deploy_s3_bucket_name
  key    = local.backend_deploy_artifact_s3_key

  source = "${path.module}/my_deployment_package.zip"

  tags = local.tags
  etag = filemd5("${path.module}/my_deployment_package.zip")
}
resource "aws_lambda_permission" "apigw_lambda" {
  provider = aws.app_local

  statement_id  = "AllowExecutionFromAPIGateway"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.lambda.function_name
  principal     = "apigateway.amazonaws.com"
  source_arn    = "${aws_api_gateway_rest_api.api.execution_arn}/*/*/*"
}
```

#### lambda_function.py

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import json
 
def lambda_handler(event, context):
    body = json.loads(event["body"])
    input = body["input"]

    # GPUの確認
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n!!! current device is {device} !!!\n")

    # モデルのダウンロード
    model_id = "inu-ai/dolly-japanese-gpt-1b"
    # モデルdolly-japanese-gpt-1bの最低メモリ要件は7GB:  1.3Bパラメータの日本語GPT-2モデルを使用した対話型のAI。VRAM(ビデオメモリ) 7GB または RAM(汎用メモリ) 7GB が必要
    # RAMは汎用型のメモリで、あらゆる処理に使われるが、VRAMは映像処理に特化されている
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

    # LLMs: langchainで上記モデルを利用する
    task = "text-generation"
    pipe = pipeline(
        task, 
        model=model,
        tokenizer=tokenizer,
        device=0,            # GPUを使うことを指定 (cuda:0と同義)
        framework='pt',      # モデルをPyTorchで読み込むことを指定
        max_new_tokens=32,
        temperature=0.1,
    )

    messages = [
        {
            "role": "system",
            "content": "あなたは献立を考えるシェフです。",
        },
        {"role": "user", "content": input},
    ]

    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.01)

    return {
        'statusCode': 200,
         'headers': {
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Allow-Origin': '任意のドメイン',
            'Access-Control-Allow-Methods': 'OPTIONS,POST,GET'
        },
        'body': outputs
    }
```

#### ビルド

```
pip install --target ./package boto3
cd package
zip -r ../my_deployment_package.zip .
zip my_deployment_package.zip lambda_function.py
```

### 参考情報

- 2023.05.09 LangChain×オープンな言語モデル(dolly-japanese-gpt-1b)使ってみる
  - https://qiita.com/sakue_103/items/8b32059629c01d0d27f2
- 2024.02.20 Hugging FaceのChat Template機能でOSSな大規模言語モデルを使ってみる
  - https://dev.classmethod.jp/articles/huggingface-chat-template-with-oss-llm/
- 2023.09.21 API Gateway/Lambda Terraformコーディング
  - https://github.com/Eigo-Mt-Fuji/efg-terraform-2022/blob/main/terraform-components/2_6_backend_api/api-gateway-lambda.tf
- APIGateway + LambdaでAPIを作る【Python編】
  - https://tool-engineer.work/article66/#toc6
- AWS Lambda 関数 x Python で .zip ファイルアーカイブを使用する
  - https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/python-package.html
- AWS Lambda クォータ
  - https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/gettingstarted-limits.html


## 次のアクション

- 「API Gateway/Lambdaを使ってオープンソースの言語モデル「dolly-japanese-gpt-1b」をホスティングしてみる」を実践
  - ブレイクダウンしたソースコードを使ってやってみる
